\input{preamble/preamble.tex}
\input{preamble/preamble-addon-listings.tex}
\usepackage{url}

\author{Dzenan Dumpor \\ Einar Baumann \\ Teodor Heggelund}
\title{TMA4280 Exercise 4}

\begin{document}
\maketitle

\clearpage
\section{Program} % (fold)
\label{sec:program}
The source for the program can be found on Github: \url{https://github.com/einar90/tma4280_gruppe}.
% section program (end)


\section{Utilizing OpenMP} % (fold)
\label{sec:utilizing_openmp}
OpenMP is enabled for the loop in the summing function through the use of a pragma:
\begin{lstlisting}
#pragma omp parallell for schedule(static) reduction(+:sum) private(i)
  for(i = start; i < stop; i++) {
    sum = sum + vector->data[i];
  }
\end{lstlisting}
% section utilizing_openmp (end)


\section{Utilizing MPI} % (fold)
\label{sec:utilizing_mpi}
MPI is used in the main-loop. Process 0 generates the entire vector and sends parts of it out to the other processes, then waits to receive the results. The other processes sum their part and send the result back to process 0.

\begin{lstlisting}
if (rank == 0){ %...
  for (int i = 1; i < size; ++i){ %...
    MPI_Send(&vector->data[(i-1)*partlen], partlen, MPI_DOUBLE, i, tag, MPI_COMM_WORLD);
  }
  for (int i = 1; i < size; ++i){ %...
    MPI_Recv(&recivedsum, 1, MPI_DOUBLE, i, 42, MPI_COMM_WORLD, &status); %...
  }
}
else { %...
  MPI_Recv(vector->data, vector->len, MPI_DOUBLE, 0, tag, MPI_COMM_WORLD, &status); %...
  MPI_Send(&partsum, 1, MPI_DOUBLE, 0, 42, MPI_COMM_WORLD);
} %...
\end{lstlisting}
% section utilizing_mpi (end)


\clearpage
\section{Convenient MPI calls} % (fold)
\label{sec:convenient_mpi_calls}
\begin{description}
  \item[MPI\_Init] Initialize MPI.
  \item[MPI\_Comm\_size] Get number of processes.
  \item[MPI\_Comm\_rank] Get rank of process.
  \item[MPI\_Comm\_dup] Duplicate communicator.
  \item[MPI\_Wtime] Returns elapsed time on the calling processor.
  \item[MPI\_Send] Send message with a tag to a specific receiver (blocking).
  \item[MPI\_Recv] Receive a message with a specific tag from a specific sender (blocking).
\end{description}
% section convenient_mpi_calls (end)


\section{Comparison of error for P=8 and P=2} % (fold)
\label{sec:comparison_of_error_for_p_8_and_p_2}
A plot of the errors for various data set sizes for 2 and 8 processors are shown in Figure~\ref{fig:error}. The lines overlap perfectly, as is tradition.

\begin{figure}[htbp]
  \centering
  \includegraphics[]{graphics/error.eps}
  \caption{A plot of the error. The two lines overlap perfectly.}
  \label{fig:error}
\end{figure}
% section comparison_of_error_for_p_8_and_p_2 (end)



\section{Memory requirement} % (fold)
\label{sec:memory_requirement}
For the root process (rank 0) the memory requirement $M_0$ is
\begin{equation}
  M_0 = 8 \, \mathrm{bytes} \times n
\end{equation}
where $n$ is the number of elements to be summed. For all other processes, the memory requirement $M_p$ per processor scales with the number of elements $n$ and the number of processors $P$ as follows:
\begin{equation}
  M_p = \frac{8 \, \mathrm{bytes} \times n}{P-1}
\end{equation}

\begin{table}[H]
  \centering
  \caption{The memory required for various numbers of processors and data set sizes. For $P>1$, the memory used by the root process is ignored (it is what's shown in the first row).}
  \label{tab:memoryreq}
  \begin{tabularx}{1.0\textwidth}{X|XXXXXX}
    \toprule
    $n$/$P$  & 8  & 32  & 128  & 1024 & 4096  & 16384 \\
    \midrule
    1        & 64 & 256 & 1024 & 8192 & 32768 & 131072 \\
    2        & 64 & 256 & 1024 & 8192 & 32768 & 131072 \\
    4        & 22 & 86  & 342  & 2731 & 10123 & 43691  \\
    8        & 10 & 37  & 147  & 1171 & 4682  & 18725  \\
    \bottomrule
  \end{tabularx}
\end{table}

% section memory_requirement (end)

\section{Counting FLOPs}
A FLOP (Floating Point Operation) is according to Wikipedia adding or multiplying two floating point numbers\cite{wikiflops}. When generating our vector we first multiply two integers, then divide 1.0 by the result. This is zero floating point operations.
Division is not counted because dividing two floating point numbers is much more expensive. Stack Overflow-user Die in Sente claims that
\begin{quote}
{\em How much longer depends on the processor, but there's sort of a defacto standard in the HPC community to count one division as 4 flops}
\end{quote}
Under this assumption, we use $4 \times n$ FLOPs to initialize the vector $\mathbf{v}$.

The summation is then distributed on the processors, and in total $n-1$ summations are calculated on all the processors, hence the summation requires $n-1 FLOPs$.


\section{Applicability of parallel computing for this problem} % (fold)
\label{sec:applicability_of_parallel_computing_for_this_problem}
This problem does not involve a large number of computations, and does not require a lot of memory. Therefore there are no good reasons to use parallel processing to solve this problem. We also see this quite clearly in Figure~\ref{fig:runtime}: the runtime increases with the number of MPI-processes, and is generally higher for 8 OpenMP threads than for 1 OpenMP thread.

\begin{figure}[H]
  \centering
  \includegraphics[]{graphics/runtime.eps}
  \caption{A plot of the runtimes for various numbers of processors and data set sizes.}
  \label{fig:runtime}
\end{figure}
% section applicability_of_parallel_computing_for_this_problem (end)

\begin{thebibliography}{1}
  \bibitem{wikiflops} Wikipedia article on \emph{FLOPS}: \url{http://en.wikipedia.org/wiki/Flops}.
\end{thebibliography}

\end{document}
