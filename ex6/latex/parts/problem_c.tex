% -*- root: ../ex6.tex -*-
\section{Performance} % (fold)
\label{sec:performance}

\subsection{Overview} % (fold)
\label{sub:overview}
What follows is a simplified overview of of the expected asymptotic run times for the different parts of our solver; as well as a simplified analysis of the network latency introduced by the \texttt{MPI\_Send/Recv} calls in the program.

\subsubsection{Asymptotic runtimes} % (fold)
\label{ssub:asymptotic_runtimes}

\begin{itemize}

  \item \texttt{transpose(a,b)} has an asymptotic runtime of $O(N^2)$. Each element in the matrix \textbf{B} has to be copied into a new matrix \textbf{A}. This is always performed by the root prosess.

  \item Both \texttt{fst} and \texttt{fstinv} have an asymptotic runtime of $O(N^2 \log(N))$. As each process has a part of the whole matrix \textbf{B} this is going to take at most $O(N_p * N\log(N))$ (where $N_p$ is the number of columns being transformed by the process) every time these transformations are performed by every process except the root process.

  \item Finding $\tilde{x}$ by applying the eigenvalues $\lambda$ is done in $O(N^2)$. This is always performed by the root process.

\end{itemize}

% For a full run..
When we apply the solver to a given dataset it performs $2\times \mathtt{fst}$, $2\times \mathtt{fstinv}$, $2\times \mathtt{transpose}$ and calculates $\tilde{x}$ once. If we run the solver with a small number of prosesses the we would expect to see a bottleneck in the \texttt{fst} and \texttt{fstinv} functions. If we have a reasonably large number of processes and a sufficiently large $n$, we would expect that the program would spend the largest amount of time computing the transpose of the matrix and $\tilde{x}$, as one process is performing \emph{all} the operations.
% subsubsection asymptotic_runtimes (end)


\subsubsection{Network latency} % (fold)
\label{ssub:network_latency}

Here we choose to use a simple linear network model where the time to send $b$ bytes is modelled as:
\begin{equation}
  T^{comm}(b) = \kappa + \gamma N^2
\end{equation}
where $\kappa$ is the network latency and $\gamma$ is the inverse network bandwith.

In our solver the root prosess has to send $N^2$ bytes and recieve $N^2$ bytes for every transform operation. The whole matrix has to be distributed across the network no matter how we set up the problem, but this could be done more efficiently by making sure every prosess is responsible for distributing $N^2/P$ bytes each.

% subsubsection network_latency (end)


% subsection overview (end)


\subsection{Measured run time in relation to the problem size $n$} % (fold)
\label{sub:run_time_in_relation_to_the_problem_size_n_}

A plot of the runtime for a constant number of processes $t\times p = 36$ and varying problem sizes $n$ is shown in in Figure~\ref{fig:runtime_const_pt}. We see that the run time of our program fits well with the curve for the predicted run time $O(N^2 \ln(N))$.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{illustrations/plots/const_pt_runtime.pdf}
  \caption{Plot of the runtime for a constant number of processes $t\times p = 36$ for varying problem sizes $n$. A plot of $N^2 \ln(N)$ is shown for reference to indicate the ``correct shape''.}
  \label{fig:runtime_const_pt}
\end{figure}
% subsection run_time_in_relation_to_the_problem_size_n_ (end)



\subsection{Comparison of measured MPI and OpenMP performance} % (fold)
\label{sub:comparison_of_mpi_and_openmp_performance}
In this section we compare the run times for $n=16,384$ and various combinations of the number of MPI processes $p$ and OpenMP threads pr. process $t$, where $p\times t = 36$. The results are shown in Table~\ref{tab:runtimes_36}.

In our case, we achieve the best performance for large $t$ and low $p$: the run time with $t=36, p=1$ is 16.3\% lower than the second best run time, with $t=18, p=2$. In the mid range we find the run times for high numbers of $p$ and low numbers of $t$. We get the highest run times for close numbers of $t$ and $p$.

The significantly lower run time for $p=1$ is likely due to the absence of \texttt{MPI\_Send/Recv} calls, which introduces a significant latency in the program.
The low run time for $p=2$ is likely due to processes staying on one node, thus reducing the send/receive latency.

The relatively good run time for $p=36$ is explained by the fact that only one process is ``reserved'' for transposing and distributing the matrix, while the 35 others are responsible for transformations. This means 35 transformations can run in parallel on different processors, while in the cases with $p=18,12,9,6$ only $(p-1)\times t = 34, 33, 32$ and $30$ threads, respectively, run in parallel.

In conclusion: out program does not utilize the hybrid model very well at all.



\begin{table}[H]
  \centering
  \caption{Measured runtimes when $p\times t = 36$, for $n=16,386$.}
  \label{tab:runtimes_36}
  \begin{tabularx}{0.5\textwidth}{XXXX}
    \toprule
    $t$ & $p$ & $\tau$ \\
    \midrule
    36  &  1  &  112.515614 \\
    18  &  2  &  134.508309 \\
    1   & 36  &  136.474492 \\
    2   & 18  &  141.108449 \\
    3   & 12  &  143.360202 \\
    4   &  9  &  147.304511 \\
    6   &  6  &  153.733214 \\
    \bottomrule
  \end{tabularx}
\end{table}

% subsection comparison_of_mpi_and_openmp_performance (end)
% section performance (end)
